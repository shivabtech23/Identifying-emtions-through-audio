ğŸ§ Speech Emotion Recognition using Audio

This project focuses on identifying human emotions from speech audio using machine learning and deep learning techniques.
The system analyzes audio signals and classifies them into one of eight emotional categories, based on how humans express emotions through voice.

The goal of this project was not just accuracy, but to understand the complete pipeline â€” from raw audio processing to model evaluation and visualization.

â¸»

ğŸ” Problem Statement

Human emotions are strongly reflected in speech patterns such as tone, pitch, and intensity.
Manually identifying emotions from audio is subjective and inconsistent.

This project aims to:
	â€¢	Automatically detect emotions from speech
	â€¢	Learn meaningful audio features
	â€¢	Build an interpretable and visual ML pipeline

â¸»

ğŸ¯ Emotions Classified

The model classifies audio into the following 8 emotions:
	â€¢	Angry
	â€¢	Calm
	â€¢	Disgust
	â€¢	Fearful
	â€¢	Happy
	â€¢	Neutral
	â€¢	Sad
	â€¢	Surprised

â¸»

ğŸ“‚ Datasets Used
	â€¢	RAVDESS â€“ Ryerson Audio-Visual Database of Emotional Speech and Song
	â€¢	TESS â€“ Toronto Emotional Speech Set

Both datasets provide labeled emotional speech samples, which were cleaned and unified before training.

â¸»

âš™ï¸ Project Workflow (High-Level)
	1.	Audio Loading
Audio files are loaded and standardized using Librosa.
	2.	Feature Extraction
Important speech characteristics are extracted using:
	â€¢	MFCCs
	â€¢	Mel Spectrograms
	3.	Preprocessing
	â€¢	Feature scaling
	â€¢	Label encoding
	â€¢	Trainâ€“test split
	4.	Model Training
	â€¢	An LSTM-based neural network is trained to classify emotions.
	5.	Evaluation & Visualization
	â€¢	Accuracy and loss tracking
	â€¢	Confusion matrix
	â€¢	Per-class accuracy analysis

â¸»

ğŸ§  Model Used
	â€¢	LSTM (Long Short-Term Memory) network
	â€¢	Chosen because speech is sequential data, and LSTM helps model temporal patterns in audio features.

â¸»

ğŸ§ª Performance Summary
	â€¢	Overall Accuracy: ~82%
	â€¢	Strong performance on:
	â€¢	Neutral
	â€¢	Angry
	â€¢	Happy

The model generalizes reasonably well given the dataset size and feature-based approach.

â¸»

ğŸ“Š Visualizations Included

This project includes multiple visualizations to better understand the data and model behavior:
	## ğŸ“Š Visualizations

### Audio Waveform
![Audio Waveform](images/waveform.png)

### Mel Spectrogram
![Mel Spectrogram](images/mel_spectrogram.png)

### Confusion Matrix
![Confusion Matrix](images/confusion_matrix.png)

### Training Curves
![Training Curves](images/training_curves.png)

### Feature Correlation Matrix
![Feature Correlation Matrix](images/feature_correlation_matrix.png)
### Per-Class Accuracy
![Per-Class Accuracy](images/per_class_accuracy.png)

These help explain why the model behaves the way it does, not just what it predicts.

â¸»

ğŸ› ï¸ Tech Stack (One-Line Each)
	â€¢	Python â€“ Core programming language
	â€¢	MFCCs â€“ Speech feature extraction
	â€¢	Mel Spectrogram â€“ Audio frequency representation
	â€¢	LSTM â€“ Sequential learning model
	â€¢	Librosa â€“ Audio processing library
	â€¢	TensorFlow / Keras â€“ Model training framework

â¸»

ğŸš€ Future Improvements
	â€¢	Use frame-level sequential features instead of mean-pooled vectors
	â€¢	Apply data augmentation (noise, pitch shift, time stretch)
	â€¢	Add early stopping and regularization to reduce overfitting

â¸»

ğŸ‘¤ Author

Shiv Arora
B.Tech (Hons.) Computer Science â€“ Cybersecurity
AI / ML & Security Projects

	â€¢	LinkedIn: https://www.linkedin.com/in/shivarora1/
	â€¢	GitHub: https://github.com/shivabtech23
